{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs682/assignments/assignment3/'\n","FOLDERNAME ='/memorization-682proj/'\n","# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# This downloads the COCO dataset to your Drive\n","# if it doesn't already exist.\n","# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs682/datasets/\n","# !bash get_datasets.sh\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4lCjLXxjV2j","executionInfo":{"status":"ok","timestamp":1701764623056,"user_tz":300,"elapsed":16486,"user":{"displayName":"Rishabh Garg","userId":"12191631028853698319"}},"outputId":"37b57e22-982f-4b70-dd94-9a3a85208826"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/memorization-682proj\n"]}]},{"cell_type":"code","source":["# !pip install datasets"],"metadata":{"id":"GYOXpUPyjvBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import (\n","    GPT2LMHeadModel,\n","    GPT2Tokenizer,\n","    DataCollatorForLanguageModeling,\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n",")\n","\n","from torch.utils.data.dataloader import default_collate\n","import numpy as np\n","from datasets import load_dataset\n","from tqdm import tqdm\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","from pprint import pprint\n","from collections import defaultdict\n","\n","\n","import nltk\n","# nltk.download('punkt')\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.tokenize import word_tokenize\n","\n","\n","\n","CACHE_DIR = \"./cache\"\n","if not os.path.exists(CACHE_DIR):\n","    os.makedirs(CACHE_DIR)\n","\n","from torch.utils.data.dataloader import default_collate\n","\n","class SummarizationDataCollator:\n","    def __call__(self, batch):\n","        # Convert each item in the batch to tensors and stack them\n","        input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n","        attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n","        labels = torch.stack([torch.tensor(item['labels']) for item in batch])\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels\n","        }\n","\n","# Load dataset\n","dataset = load_dataset(\"cnn_dailymail\", '3.0.0',cache_dir=CACHE_DIR)\n","# Define constants /args\n","text_column='article'\n","summary_column='highlights'\n","max_source_length=100\n","max_target_length=100\n","ignore_pad_token_for_loss=True\n","train_batch_size=16\n","val_batch_size=8\n","\n","# Tokenize function\n","def preprocess_function(examples):\n","    # print(\"Original:\", examples)\n","\n","    inputs = examples[text_column]\n","    targets = examples[summary_column]\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding='max_length', truncation=True)\n","\n","    # Tokenize targets\n","    labels = tokenizer(targets, max_length=max_target_length, padding='max_length', truncation=True)\n","    if ignore_pad_token_for_loss:\n","        # Replace pad token id (-100) where appropriate\n","        labels[\"input_ids\"] = [\n","            label if label != tokenizer.pad_token_id else -100 for label in labels[\"input_ids\"]\n","        ]\n","    # Replace padding token id in labels with -100 if ignoring pad token for loss\n","    # if ignore_pad_token_for_loss:\n","    #     labels[\"input_ids\"] = [\n","    #         [(label if label != tokenizer.pad_token_id else -100) for label in label_ids] for label_ids in labels[\"input_ids\"]\n","    #     ]\n","\n","    return {\n","        \"input_ids\": model_inputs[\"input_ids\"],\n","        \"attention_mask\": model_inputs[\"attention_mask\"],\n","        \"labels\": labels[\"input_ids\"]\n","    }\n","\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# sample_data = dataset['train'].select(range(50))\n","\n"],"metadata":{"id":"ZjqzQWCdjW1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Tokenize dataset\n","tokenized_dataset_path = os.path.join(CACHE_DIR, \"tokenized_dataset.pt\")\n","# print(tokenized_dataset_path)\n","if os.path.exists(tokenized_dataset_path):\n","    tokenized_datasets = torch.load(tokenized_dataset_path)\n","else:\n","    # Tokenize and cache dataset\n","  tokenized_datasets = dataset.map(preprocess_function, batched=True,load_from_cache_file=True)\n","  print(f'Saving tokenized dataset in this path {tokenized_dataset_path}')\n","  torch.save(tokenized_datasets, tokenized_dataset_path)\n","# tokenized_datasets = dataset.map()\n","# for i, example in enumerate(tokenized_datasets):\n","#     print(f\"Example {i}: {example}\")\n","#     if i >= 2:  # Inspect only the first few examples\n","#         break\n","print(\"Dataset Columns and Keys:\")\n","print(tokenized_datasets)\n","# Print columns for each split (e.g., train, validation, test)\n","# for split in tokenized_datasets.keys():\n","#     print(f\"\\n{split} Split:\")\n","#     # Print column names\n","#     print(\"Columns:\", tokenized_datasets[split].column_names)\n","\n","#     # Optionally, print a few example keys (IDs) from the dataset\n","#     print(\"Example Keys:\", [tokenized_datasets[split][i]['id'] for i in range(3)])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdTtxN3rlDAE","executionInfo":{"status":"ok","timestamp":1701766157547,"user_tz":300,"elapsed":152,"user":{"displayName":"Rishabh Garg","userId":"12191631028853698319"}},"outputId":"53dd0c1c-69c9-48dd-c6fe-abfc95361d58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Columns and Keys:\n","DatasetDict({\n","    train: Dataset({\n","        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 287113\n","    })\n","    validation: Dataset({\n","        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 13368\n","    })\n","    test: Dataset({\n","        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 11490\n","    })\n","})\n"]}]},{"cell_type":"code","source":["class MemorisationDataset(Dataset):\n","    def __init__(self, prefix_file, suffix_file):\n","        self.prefixes = np.load(prefix_file).astype(np.int64)  # Convert to int64\n","        self.suffixes = np.load(suffix_file).astype(np.int64)\n","\n","    def __len__(self):\n","        return len(self.prefixes)\n","\n","    def __getitem__(self, idx):\n","        return self.prefixes[idx], self.suffixes[idx]\n","\n","\n","def load_lmdataset():\n","    print(f'Loading dataset from ./data/ folder')\n","    # train_prefix = np.load('./data/train_prefix.npy')\n","    # train_suffix = np.load('./data/train_suffix.npy')\n","    train_preprefix = np.load('./data/train_preprefix.npy')\n","    train_dataset = np.load('./data/train_dataset.npy')\n","    dataset = MemorisationDataset('./data/train_prefix.npy', './data/train_suffix.npy')\n","\n","    return dataset,train_preprefix,train_dataset\n","\n","\n","def calculate_bleu_score(references, candidates):\n","    score = 0\n","    for ref, cand in zip(references, candidates):\n","        ref_tokens = [word_tokenize(ref)]\n","        cand_tokens = word_tokenize(cand)\n","        score += sentence_bleu(ref_tokens, cand_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n","    return score / len(references)\n","\n","def load_tokenizer_for_causal_lm(model_name):\n","    \"\"\"\n","    Load tokenizer with required config changes\n","    \"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # For Autoregressive models, padding on the right would mean the model\n","    # will receive padded tokens as context, which is not useful during generation\n","    tokenizer.padding_side = 'left'\n","    tokenizer.pad_token = tokenizer.eos_token\n","    return tokenizer"],"metadata":{"id":"8VnX_wceFZwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","# elif torch.backends.mps.is_available():\n","#     device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(\"Using device:\", device)\n","# DataLoader\n","data_collator = SummarizationDataCollator()\n","\n","\n","train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=train_batch_size,collate_fn=data_collator)\n","val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=val_batch_size,collate_fn=data_collator)\n","\n","# Load model\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model = model.to(device)\n","\n","# Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","\n","# Training loop\n","epochs = 5\n","\n","# for i, batch in enumerate(train_dataloader):\n","#     print(f\"Batch {i}: input_ids shape - {batch['input_ids'].shape}, attention_mask shape - {batch['attention_mask'].shape}\")\n","#     if i >= 2:  # Inspect only the first few batches\n","#         break\n","\n","\n","tokenizer_gpt2 = load_tokenizer_for_causal_lm(\"gpt2\")\n","print(\"Loaded tokenizer for mem dataset\",tokenizer_gpt2)\n","\n","def preprocess_dataset(dataset):\n","    decoded_prefixes = [tokenizer_gpt2.decode(prefix) for prefix in dataset.prefixes]\n","    decoded_suffixes = [tokenizer_gpt2.decode(suffix) for suffix in dataset.suffixes]\n","    return list(zip(decoded_prefixes, decoded_suffixes))\n","\n","# Top k sampling\n","top_k = 40\n","max_length_prefix=50\n","max_length_suffix=50\n","\n","evalbatch_size=16\n","\n","print(\"Loading LM Extraction eval dataset\")\n","evaldataset,train_preprefix,train_dataset=load_lmdataset()\n","preprocessed_data = preprocess_dataset(evaldataset)\n","# evaldata_loader = DataLoader(evaldataset, batch_size=evalbatch_size, shuffle=False)\n","evaldata_loader = DataLoader(preprocessed_data, batch_size=evalbatch_size, shuffle=False)\n","\n","bleu_scores = []\n","\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in tqdm(train_dataloader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        inputs = batch[\"input_ids\"]\n","        labels = batch['labels']\n","        outputs = model( inputs, labels=labels)\n","        loss = outputs.loss\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Save checkpoint\n","    torch.save(model.state_dict(), f\"gpt2_cnn_dailymail_epoch{epoch}.pt\")\n","\n","    #Memorization eval\n","    total_bleu_score=0\n","    total_samples=0\n","    model.eval()\n","    with torch.no_grad():\n","      for i, batch in enumerate(tqdm(evaldata_loader)):\n","          input_len = 10\n","          # prompts = []\n","          # input_ids = []\n","          # attention_mask = []\n","          prefixes, true_suffixes = batch\n","          # decoded_prefixes = [tokenizer_gpt2.decode(prefix) for prefix in prefixes.numpy()]\n","          # decoded_true_suffixes = [tokenizer_gpt2.decode(suffix) for suffix in true_suffixes.numpy()]\n","\n","          inputs = tokenizer(prefixes, return_tensors='pt', padding=True).to(device)\n","\n","          generated_sequences = model.generate(\n","              input_ids = inputs['input_ids'],\n","              attention_mask = inputs['attention_mask'],\n","              max_length = max_length_prefix+max_length_suffix,\n","              do_sample = True,\n","              top_k = top_k,\n","              top_p = 1.0\n","          )\n","          generated_texts = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n","\n","          generated_suffixes = [text[len(prefix):] for text, prefix in zip(generated_texts,prefixes)]\n","\n","          bleu_score = calculate_bleu_score(true_suffixes, generated_suffixes)\n","          total_bleu_score += bleu_score\n","          print(f'Batch {i} bleu score {bleu_score}')\n","          total_samples+=1\n","\n","    avg_bleu_score = total_bleu_score / total_samples\n","    bleu_scores.append(avg_bleu_score)\n","\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            inputs = batch[\"input_ids\"].to(device)\n","            labels = inputs.clone()\n","            outputs = model(inputs, labels=labels)\n","            total_loss += outputs.loss.item()\n","\n","    print(f\"Validation Loss after Epoch {epoch}: {total_loss / len(val_dataloader)}\")\n","\n","# Save final model\n","torch.save(model.state_dict(), \"gpt2_cnn_dailymail_final.pt\")\n","np.save(\"bleu_scores.npy\", np.array(bleu_scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTIS77YFjo3e","outputId":"94d34d9e-0a0b-4824-9d66-260e88e8097d"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Loaded tokenizer for mem dataset GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","}\n","Loading LM Extraction eval dataset\n","Loading dataset from ./data/ folder\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 6402/17945 [46:22<1:23:41,  2.30it/s]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gYXcDBw9wHom"},"execution_count":null,"outputs":[]}]}