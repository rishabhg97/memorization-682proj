# -*- coding: utf-8 -*-
"""Finetune-opt-bnb-peft IMDB with Custom Trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QlAcmG4TLHFl_2i38SOaJEjRu5KJYiIy

## Fine-tune large models using ðŸ¤— `peft` adapters, `transformers` & `bitsandbytes`

In this tutorial we will cover how we can fine-tune large language models using the very recent `peft` library and `bitsandbytes` for loading large models in 8-bit.
The fine-tuning method will rely on a recent method called "Low Rank Adapters" (LoRA), instead of fine-tuning the entire model you just have to fine-tune these adapters and load them properly inside the model.
After fine-tuning the model you can also share your adapters on the ðŸ¤— Hub and load them very easily. Let's get started!

### Install requirements

First, run the cells below to install the requirements:
"""

!pip install -q bitsandbytes datasets accelerate loralib
!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git
!pip install evaluate

"""### Model loading

Here let's load the `opt-6.7b` model, its weights in half-precision (float16) are about 13GB on the Hub! If we load them in 8-bit we would require around 7GB of memory instead.
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
import torch
import torch.nn as nn
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-6.7b",
    load_in_8bit=True,
    device_map='auto',
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-6.7b")

"""### Post-processing on the model

Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in `float32` for stability. We also cast the output of the last layer in `float32` for the same reasons.
"""

for param in model.parameters():
  param.requires_grad = False  # freeze the model - train adapters later
  if param.ndim == 1:
    # cast the small parameters (e.g. layernorm) to fp32 for stability
    param.data = param.data.to(torch.float32)

model.gradient_checkpointing_enable()  # reduce number of stored activations
model.enable_input_require_grads()

class CastOutputToFloat(nn.Sequential):
  def forward(self, x): return super().forward(x).to(torch.float32)
model.lm_head = CastOutputToFloat(model.lm_head)

"""### Apply LoRA

Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`.
"""

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="TOKEN_CLS"
)

model = get_peft_model(model, config)
print_trainable_parameters(model)

"""Set up Custom Trainer"""

import transformers
from datasets import load_dataset


class CustomTrainer(transformers.Trainer):
  def evaluate(self, eval_dataset=None):
        # You can either pass the evaluation dataset or use the one already set in the trainer
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset

        # Your custom evaluation logic here
        # For example, use the model to generate predictions on eval_dataset
        # and then calculate the BLEU score with the ground truths
        metrics = custom_evaluation_function(self.model, eval_dataset)

        return metrics


def custom_evaluation_function(model, dataset):
    # Load dataset, generate predictions, calculate BLEU scores
    # Return a dictionary with the calculated metrics
    pass

"""### Training"""

"""import transformers
from datasets import load_dataset
data = load_dataset("imdb") #Dataset
data = data.map(lambda samples: tokenizer(samples['text']), batched=True) #Text column

trainer = transformers.Trainer(
    model=model,
    train_dataset=data['train'],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        max_steps=200,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=1,
        output_dir='outputs'
    ),
    data_collator=transformers.DataCollatorForTokenClassification(tokenizer) #Change to fit the type of transformer
)
model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()"""


# Load the IMDb dataset
data = load_dataset("imdb")

# Load a pre-trained tokenizer and tokenize the text column
tokenizer = transformers.AutoTokenizer.from_pretrained("bert-base-uncased")
data = data.map(lambda samples: tokenizer(samples['text'], truncation=True, padding=True), batched=True)

# Load a pre-trained model for sequence classification
model = transformers.AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
import numpy as np
import evaluate
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
# Initialize the Trainer
trainer = CustomTrainer(
    model=model,
    train_dataset=data['train'],
    compute_metrics=compute_metrics,
    args=transformers.TrainingArguments(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        max_steps=200,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=1,
        output_dir='outputs',
    ),
    data_collator=transformers.DataCollatorWithPadding(tokenizer)  # Use DataCollatorWithPadding for sequence classification
)

# Train the model
trainer.train()