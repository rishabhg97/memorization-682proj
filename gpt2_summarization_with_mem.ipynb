{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhg97/memorization-682proj/blob/main/gpt2_summarization_with_mem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # This mounts your Google Drive to the Colab VM.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# # assignment folder, e.g. 'cs682/assignments/assignment3/'\n",
        "# FOLDERNAME ='/memorization-682proj/'\n",
        "# # assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# # Now that we've mounted your Drive, this ensures that\n",
        "# # the Python interpreter of the Colab VM can load\n",
        "# # python files from within it.\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# # This downloads the COCO dataset to your Drive\n",
        "# # if it doesn't already exist.\n",
        "# # %cd /content/drive/My\\ Drive/$FOLDERNAME/cs682/datasets/\n",
        "# # !bash get_datasets.sh\n",
        "# %cd /content/drive/My\\ Drive/$FOLDERNAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4lCjLXxjV2j",
        "outputId": "37b57e22-982f-4b70-dd94-9a3a85208826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/memorization-682proj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone git@github.com:rishabhg97/memorization-682proj.git\n",
        "%cd memorization-682proj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BpHSpCjwTyZ",
        "outputId": "d80d3c2e-0c7c-45a1-9916-e8201b92973a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/memorization-682proj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwZ3wPFy5Kiy",
        "outputId": "f5a61352-e52e-4908-ea81-89c9902c1c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fatal: not a git repository (or any of the parent directories): .git']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets transformers==4.24.0\n",
        "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n"
      ],
      "metadata": {
        "id": "GYOXpUPyjvBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "CACHE_DIR = \"./cache\"\n",
        "if not os.path.exists(CACHE_DIR):\n",
        "    os.makedirs(CACHE_DIR)\n",
        "\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "class SummarizationDataCollator:\n",
        "    def __call__(self, batch):\n",
        "        # Convert each item in the batch to tensors and stack them\n",
        "        input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n",
        "        attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n",
        "        labels = torch.stack([torch.tensor(item['labels']) for item in batch])\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0',cache_dir=CACHE_DIR)\n",
        "# Define constants /args\n",
        "text_column='article'\n",
        "summary_column='highlights'\n",
        "max_source_length=100\n",
        "max_target_length=100\n",
        "ignore_pad_token_for_loss=True\n",
        "train_batch_size=16\n",
        "val_batch_size=8\n",
        "\n",
        "# Tokenize function\n",
        "def preprocess_function(examples):\n",
        "    # print(\"Original:\", examples)\n",
        "\n",
        "    inputs = examples[text_column]\n",
        "    targets = examples[summary_column]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding='max_length', truncation=True)\n",
        "\n",
        "    # Tokenize targets\n",
        "    labels = tokenizer(targets, max_length=max_target_length, padding='max_length', truncation=True)\n",
        "    if ignore_pad_token_for_loss:\n",
        "        # Replace pad token id (-100) where appropriate\n",
        "        labels[\"input_ids\"] = [\n",
        "            label if label != tokenizer.pad_token_id else -100 for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    # Replace padding token id in labels with -100 if ignoring pad token for loss\n",
        "    # if ignore_pad_token_for_loss:\n",
        "    #     labels[\"input_ids\"] = [\n",
        "    #         [(label if label != tokenizer.pad_token_id else -100) for label in label_ids] for label_ids in labels[\"input_ids\"]\n",
        "    #     ]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": model_inputs[\"input_ids\"],\n",
        "        \"attention_mask\": model_inputs[\"attention_mask\"],\n",
        "        \"labels\": labels[\"input_ids\"]\n",
        "    }\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# sample_data = dataset['train'].select(range(50))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZjqzQWCdjW1i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt-56BtAaAv-",
        "outputId": "61114afd-ed5d-45f5-9fe3-a23a928f60ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom_trainer.py\t\tgpt2_QA_answering.ipynb        models\n",
            "data\t\t\t\tgpt2_sentiment_analysis.ipynb  opt_finetune.py\n",
            "GPT2.ipynb\t\t\tgpt2_summarization.ipynb       OPT.ipynb\n",
            "gpt2_machine_translation.ipynb\tmemorize.py\t\t       README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize dataset\n",
        "tokenized_dataset_path = os.path.join(CACHE_DIR, \"tokenized_dataset.pt\")\n",
        "# print(tokenized_dataset_path)\n",
        "if os.path.exists(tokenized_dataset_path):\n",
        "    tokenized_datasets = torch.load(tokenized_dataset_path)\n",
        "else:\n",
        "    # Tokenize and cache dataset\n",
        "  tokenized_datasets = dataset.map(preprocess_function, batched=True,load_from_cache_file=False)\n",
        "  print(f'Saving tokenized dataset in this path {tokenized_dataset_path}')\n",
        "  torch.save(tokenized_datasets, tokenized_dataset_path)\n",
        "# tokenized_datasets = dataset.map()\n",
        "# for i, example in enumerate(tokenized_datasets):\n",
        "#     print(f\"Example {i}: {example}\")\n",
        "#     if i >= 2:  # Inspect only the first few examples\n",
        "#         break\n",
        "print(\"Dataset Columns and Keys:\")\n",
        "print(tokenized_datasets)\n",
        "# Print columns for each split (e.g., train, validation, test)\n",
        "# for split in tokenized_datasets.keys():\n",
        "#     print(f\"\\n{split} Split:\")\n",
        "#     # Print column names\n",
        "#     print(\"Columns:\", tokenized_datasets[split].column_names)\n",
        "\n",
        "#     # Optionally, print a few example keys (IDs) from the dataset\n",
        "#     print(\"Example Keys:\", [tokenized_datasets[split][i]['id'] for i in range(3)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdTtxN3rlDAE",
        "outputId": "889ad7dc-02b5-4564-e92e-846e3da54153"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Columns and Keys:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 287113\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 13368\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 11490\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MemorisationDataset(Dataset):\n",
        "    def __init__(self, prefix_file, suffix_file):\n",
        "        self.prefixes = np.load(prefix_file).astype(np.int64)  # Convert to int64\n",
        "        self.suffixes = np.load(suffix_file).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prefixes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.prefixes[idx], self.suffixes[idx]\n",
        "\n",
        "\n",
        "def load_lmdataset():\n",
        "    print(f'Loading dataset from ./data/ folder')\n",
        "    # train_prefix = np.load('./data/train_prefix.npy')\n",
        "    # train_suffix = np.load('./data/train_suffix.npy')\n",
        "    train_preprefix = np.load('./data/train_preprefix.npy')\n",
        "    train_dataset = np.load('./data/train_dataset.npy')\n",
        "    dataset = MemorisationDataset('./data/train_prefix.npy', './data/train_suffix.npy')\n",
        "\n",
        "    return dataset,train_preprefix,train_dataset\n",
        "\n",
        "\n",
        "def calculate_bleu_score(references, candidates):\n",
        "    score = 0\n",
        "    for ref, cand in zip(references, candidates):\n",
        "        ref_tokens = [word_tokenize(ref)]\n",
        "        cand_tokens = word_tokenize(cand)\n",
        "        score += sentence_bleu(ref_tokens, cand_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    return score / len(references)\n",
        "\n",
        "def load_tokenizer_for_causal_lm(model_name):\n",
        "    \"\"\"\n",
        "    Load tokenizer with required config changes\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # For Autoregressive models, padding on the right would mean the model\n",
        "    # will receive padded tokens as context, which is not useful during generation\n",
        "    tokenizer.padding_side = 'left'\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "8VnX_wceFZwr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "# elif torch.backends.mps.is_available():\n",
        "#     device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "# DataLoader\n",
        "data_collator = SummarizationDataCollator()\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=train_batch_size,collate_fn=data_collator)\n",
        "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=val_batch_size,collate_fn=data_collator)\n",
        "\n",
        "# Load model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "\n",
        "# for i, batch in enumerate(train_dataloader):\n",
        "#     print(f\"Batch {i}: input_ids shape - {batch['input_ids'].shape}, attention_mask shape - {batch['attention_mask'].shape}\")\n",
        "#     if i >= 2:  # Inspect only the first few batches\n",
        "#         break\n",
        "\n",
        "\n",
        "tokenizer_gpt2 = load_tokenizer_for_causal_lm(\"gpt2\")\n",
        "print(\"Loaded tokenizer for mem dataset\",tokenizer_gpt2)\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    decoded_prefixes = [tokenizer_gpt2.decode(prefix) for prefix in dataset.prefixes]\n",
        "    decoded_suffixes = [tokenizer_gpt2.decode(suffix) for suffix in dataset.suffixes]\n",
        "    return list(zip(decoded_prefixes, decoded_suffixes))\n",
        "\n",
        "# Top k sampling\n",
        "top_k = 40\n",
        "max_length_prefix=50\n",
        "max_length_suffix=50\n",
        "\n",
        "evalbatch_size=16\n",
        "\n",
        "print(\"Loading LM Extraction eval dataset\")\n",
        "evaldataset,train_preprefix,train_dataset=load_lmdataset()\n",
        "preprocessed_data = preprocess_dataset(evaldataset)\n",
        "# evaldata_loader = DataLoader(evaldataset, batch_size=evalbatch_size, shuffle=False)\n",
        "evaldata_loader = DataLoader(preprocessed_data, batch_size=evalbatch_size, shuffle=False)\n",
        "\n",
        "bleu_scores = []\n",
        "# test_iters=10\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i,batch in enumerate(tqdm(train_dataloader,desc=\"Train Loop\")):\n",
        "        if test_iters is not None and i> test_iters:\n",
        "            break\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        labels = batch['labels']\n",
        "        outputs = model( inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Save checkpoint\n",
        "    # model.save_pretrained(f\"./models/gpt2_cnn_dailymail_epoch{epoch}.pt\")\n",
        "    # tokenizer.save_pretrained(f\"./models/gpt2tok_cnn_dailymail_epoch{epoch}.pt\")\n",
        "    # torch.save(model.state_dict(), )\n",
        "\n",
        "    #Memorization eval\n",
        "    total_bleu_score=0\n",
        "    total_samples=0\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(tqdm(evaldata_loader,desc=\"Memorization Loop\")):\n",
        "\n",
        "          input_len = 10\n",
        "          # prompts = []\n",
        "          # input_ids = []\n",
        "          # attention_mask = []\n",
        "          prefixes, true_suffixes = batch\n",
        "          # decoded_prefixes = [tokenizer_gpt2.decode(prefix) for prefix in prefixes.numpy()]\n",
        "          # decoded_true_suffixes = [tokenizer_gpt2.decode(suffix) for suffix in true_suffixes.numpy()]\n",
        "\n",
        "          inputs = tokenizer(prefixes, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "          generated_sequences = model.generate(\n",
        "              input_ids = inputs['input_ids'],\n",
        "              attention_mask = inputs['attention_mask'],\n",
        "              pad_token_id=tokenizer.eos_token_id,\n",
        "              max_length = max_length_prefix+max_length_suffix,\n",
        "              do_sample = True,\n",
        "              top_k = top_k,\n",
        "              top_p = 1.0\n",
        "          )\n",
        "          generated_texts = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
        "\n",
        "          generated_suffixes = [text[len(prefix):] for text, prefix in zip(generated_texts,prefixes)]\n",
        "\n",
        "          bleu_score = calculate_bleu_score(true_suffixes, generated_suffixes)\n",
        "          total_bleu_score += bleu_score\n",
        "          # print(f'Batch {i} bleu score {bleu_score}')\n",
        "          total_samples+=1\n",
        "\n",
        "    avg_bleu_score = total_bleu_score / total_samples\n",
        "    bleu_scores.append(avg_bleu_score)\n",
        "\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader,desc=\"Validation Loop\"):\n",
        "            inputs = batch[\"input_ids\"].to(device)\n",
        "            labels = inputs.clone()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    print(f\"Validation Loss after Epoch {epoch}: {total_loss / len(val_dataloader)}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), \"gpt2_cnn_dailymail_final.pt\")\n",
        "np.save(\"bleu_scores.npy\", np.array(bleu_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTIS77YFjo3e",
        "outputId": "6f0aa00f-89cb-4216-925f-9b1b1ada49c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded tokenizer for mem dataset PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})\n",
            "Loading LM Extraction eval dataset\n",
            "Loading dataset from ./data/ folder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Loop:   0%|          | 11/17945 [00:04<2:11:45,  2.27it/s]\n",
            "Memorization Loop:   0%|          | 0/938 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "Memorization Loop:  11%|█         | 105/938 [00:32<04:17,  3.24it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYXcDBw9wHom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "outputId": "06750937-e06b-4dfd-dcda-4471cae8b8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m712.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2023.7.22)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ibu4RdAc0Ur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}